# coding=utf-8
'''
â€‹â€Œâ€Œâ€Œâ¡â¢â£â£â„•ğ•–ğ•¥ğ•¨ğ• ğ•£ğ•œ.ğ•¡ğ•ªâ¡â€‹

This module implements â¡â£â¢â£ğ™ğ™©ğ™¤ğ™˜ğ™ğ™–ğ™¨ğ™©ğ™ğ™˜ ğ™‚ğ™§ğ™–ğ™™ğ™ğ™šğ™£ğ™© ğ˜¿ğ™šğ™¨ğ™˜ğ™šğ™£ğ™© â¡learning algorithm for a ğ˜§ğ˜¦ğ˜¦ğ˜¥ğ˜§ğ˜°ğ˜³ğ˜¸ğ˜¢ğ˜³ğ˜¥ ğ˜¯ğ˜¦ğ˜¶ğ˜³ğ˜¢ğ˜­ ğ˜¯ğ˜¦ğ˜µğ˜¸ğ˜°ğ˜³ğ˜¬.
â€‹â€Œâ€â€Œâ¡â¢â£â¢ğ—œğ—ºğ—½ğ—¼ğ—¿ğ˜ğ—®ğ—»ğ˜:â€‹â¡ Gradients are calculated using backpropagation.
â¡â£â£â¢NÍŸoÍŸtÍŸeÍŸ: â¡This code is simple and easily modifiable, but it is not optimized
and omits many desirable features.


â€‹â€Œâ€â€Œâ¡â£â¢â£â€â€ğ—›ğ—¼ğ˜„ ğ˜ğ—¼ ğ˜‚ğ˜€ğ—²?â€‹â¡â¡â¡â€‹
â–º net = network.Network(â¡â¢â¢â£<ğ˜¯ğ˜¦ğ˜¶ğ˜³ğ˜°ğ˜¯ğ˜´ ğ˜±ğ˜¦ğ˜³ ğ˜­ğ˜¢ğ˜ºğ˜¦ğ˜³>â¡)
â–º net.SGD(â¡â¢â¢â£<ğ˜µğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜¢ğ˜µğ˜¢>â¡,â¡â¢â¢â£ <ğ˜¦ğ˜±ğ˜°ğ˜¤ğ˜©ğ˜´>â¡, â¡â¢â¢â£<ğ˜®ğ˜ªğ˜¯ğ˜ª_ğ˜£ğ˜¢ğ˜µğ˜¤ğ˜©_ğ˜´ğ˜ªğ˜»ğ˜¦>â¡, â¡â¢â¢â£<ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ğ˜ªğ˜¯ğ˜¨_ğ˜³ğ˜¢ğ˜µğ˜¦>â¡)
â–º net.evaluate(â¡â¢â¢â£<ğ˜¥ğ˜¢ğ˜µğ˜¢>â¡)

'''


"""
Network.py
~~~~~~~~~~

This module implements Stochastic Gradient Descent learning algorithm for a 
feedforward neural network.
Important: Gradients are calculated using backpropagation.   
Note: The code is simple, easily readable, and easily modifiable. It is not optimized,
and omits many desirable features.

How to use?
- net = network.Network(<neurons per layer>)
- net.SGD(<training data>, <epochs>, <mini_batch_size>, <learning_rate>)
- net.evaluate(<data>)
"""

